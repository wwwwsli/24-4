import feedparser
import streamlit as st
from datetime import datetime
import time
from openai import OpenAI

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ä¸ªæ€§åŒ–ä¿¡æ¯æ€»ç»“åŠ©æ‰‹",
    page_icon="ğŸ“š",
    layout="wide"
)

# æ ‡é¢˜
st.title("ğŸ“š ä¸ªæ€§åŒ–ä¿¡æ¯æ€»ç»“åŠ©æ‰‹")
st.markdown("---")

# ArXiv RSS æºé…ç½®
ARXIV_RSS_URL = "http://export.arxiv.org/rss/cs.AI"  # AI é¢†åŸŸçš„ RSS
KEYWORDS = ["Artificial Intelligence", "Machine Learning", "Deep Learning"]

# æ™ºè°± AI æ€»ç»“å‡½æ•°
def summarize_text(text, api_key):
    """
    è°ƒç”¨æ™ºè°± AI è¿›è¡Œæ–‡æœ¬æ€»ç»“

    Args:
        text (str): éœ€è¦æ€»ç»“çš„æ–‡æœ¬
        api_key (str): æ™ºè°± AI API Key

    Returns:
        str: æ€»ç»“åçš„æ–‡æœ¬ï¼Œå¤±è´¥æ—¶è¿”å› None
    """
    if not api_key:
        return None

    try:
        # åˆå§‹åŒ–æ™ºè°± AI å®¢æˆ·ç«¯ï¼ˆOpenAI å…¼å®¹æ¨¡å¼ï¼‰
        client = OpenAI(
            api_key=api_key,
            base_url="https://open.bigmodel.cn/api/paas/v4/"
        )

        # è°ƒç”¨ API
        response = client.chat.completions.create(
            model="glm-4",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç§‘ç ”è®ºæ–‡åŠ©æ‰‹ã€‚è¯·å°†è¿™æ®µæ‘˜è¦ç¿»è¯‘æˆé€šé¡ºçš„ä¸­æ–‡ï¼Œå¹¶ä»¥ bullet points çš„å½¢å¼åˆ—å‡º 3 æ¡æ ¸å¿ƒåˆ›æ–°ç‚¹ã€‚"
                },
                {
                    "role": "user",
                    "content": text
                }
            ],
            temperature=0.7,
            max_tokens=1000
        )

        # æå–è¿”å›çš„æ€»ç»“
        summary = response.choices[0].message.content
        return summary

    except Exception as e:
        raise Exception(f"æ™ºè°± API è°ƒç”¨å¤±è´¥: {str(e)}")

def fetch_arxiv_papers():
    """
    æŠ“å– ArXiv çš„ RSS è®¢é˜…æº

    Returns:
        list: è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«æ ‡é¢˜ã€ä½œè€…ã€æ‘˜è¦ã€å‘å¸ƒæ—¥æœŸç­‰ä¿¡æ¯
    """
    try:
        # è§£æ RSS æº
        feed = feedparser.parse(ARXIV_RSS_URL)

        papers = []

        for entry in feed.entries:
            paper = {
                'title': entry.title,
                'authors': [author.name for author in entry.authors] if hasattr(entry, 'authors') else [],
                'summary': entry.summary,
                'published_date': entry.published,
                'link': entry.link,
                'categories': entry.tags if hasattr(entry, 'tags') else []
            }
            papers.append(paper)

        return papers

    except Exception as e:
        st.error(f"æŠ“å– ArXiv è®ºæ–‡æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return []

def display_paper(paper, api_key):
    """
    æ˜¾ç¤ºå•ç¯‡è®ºæ–‡çš„ä¿¡æ¯

    Args:
        paper (dict): è®ºæ–‡ä¿¡æ¯å­—å…¸
        api_key (str): æ™ºè°± AI API Key
    """
    with st.expander(f"**{paper['title'][:100]}{'...' if len(paper['title']) > 100 else ''}**"):
        # æ ‡é¢˜
        st.markdown(f"### ğŸ“– {paper['title']}")

        # ä½œè€…å’Œæ—¥æœŸ
        authors_str = ", ".join(paper['authors'][:3])  # åªæ˜¾ç¤ºå‰3ä½ä½œè€…
        if len(paper['authors']) > 3:
            authors_str += f" ç­‰ ({len(paper['authors'])} ä½ä½œè€…)"

        col1, col2 = st.columns([3, 1])
        with col1:
            st.markdown(f"**ğŸ‘¤ ä½œè€…**: {authors_str}")
        with col2:
            if paper['published_date']:
                try:
                    pub_date = datetime.strptime(paper['published_date'], '%a, %d %b %Y %H:%M:%S %Z')
                    st.markdown(f"**ğŸ“… å‘å¸ƒ**: {pub_date.strftime('%Y-%m-%d')}")
                except:
                    st.markdown(f"**ğŸ“… å‘å¸ƒ**: {paper['published_date'][:10]}")

        # é“¾æ¥
        st.markdown(f"**ğŸ”— [åŸæ–‡é“¾æ¥]({paper['link']})**")

        # æ‘˜è¦
        st.markdown("#### ğŸ“„ æ‘˜è¦")
        st.write(paper['summary'])

        # AI æ€»ç»“
        st.markdown("#### ğŸ¤– AI æ€»ç»“")
        if not api_key:
            st.warning("âš ï¸ è¯·åœ¨ä¾§è¾¹æ å¡«å†™æ™ºè°± AI API Key ä»¥å¯ç”¨ AI æ€»ç»“åŠŸèƒ½")
        else:
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ€»ç»“..."):
                try:
                    summary = summarize_text(paper['summary'], api_key)
                    if summary:
                        st.write(summary)
                    else:
                        st.warning("âš ï¸ æ€»ç»“ç”Ÿæˆå¤±è´¥")
                except Exception as e:
                    st.error(f"âŒ {str(e)}")

# ä¸»ç•Œé¢
def main():
    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ é…ç½®")

        # æ™ºè°± AI API Key è¾“å…¥
        st.subheader("ğŸ”‘ API è®¾ç½®")
        api_key = st.text_input(
            "æ™ºè°± AI API Key",
            type="password",
            help="è¯·è¾“å…¥æ‚¨çš„æ™ºè°± AI API Key ä»¥å¯ç”¨ AI æ€»ç»“åŠŸèƒ½"
        )

        # æ˜¾ç¤ºæŠ“å–çš„è®ºæ–‡æ•°é‡
        st.subheader("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
        st.metric("å…³é”®è¯", ", ".join(KEYWORDS))
        st.metric("RSS æº", ARXIV_RSS_URL)

        # åˆ·æ–°æŒ‰é’®
        if st.button("ğŸ”„ åˆ·æ–°è®ºæ–‡", type="primary"):
            st.rerun()

    # ä¸»ä½“å†…å®¹
    st.header("ğŸ“‹ æœ€æ–° AI è®ºæ–‡")

    # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
    with st.spinner("æ­£åœ¨æŠ“å– ArXiv æœ€æ–°è®ºæ–‡..."):
        papers = fetch_arxiv_papers()

    # æ˜¾ç¤ºè®ºæ–‡æ•°é‡
    st.info(f"æ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡")

    # æ˜¾ç¤ºè®ºæ–‡åˆ—è¡¨
    if papers:
        # æœç´¢æ¡†
        search_term = st.text_input("ğŸ” æœç´¢è®ºæ–‡æ ‡é¢˜æˆ–æ‘˜è¦", "")

        # è¿‡æ»¤è®ºæ–‡
        filtered_papers = papers
        if search_term:
            filtered_papers = [
                paper for paper in papers
                if search_term.lower() in paper['title'].lower() or
                   search_term.lower() in paper['summary'].lower()
            ]
            st.info(f"æ‰¾åˆ° {len(filtered_papers)} ç¯‡åŒ¹é…çš„è®ºæ–‡")

        # æ˜¾ç¤ºè®ºæ–‡
        for paper in filtered_papers:
            display_paper(paper, api_key)
    else:
        st.warning("æœªèƒ½è·å–åˆ°è®ºæ–‡æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚")

# è¿è¡Œåº”ç”¨
if __name__ == "__main__":
    main()